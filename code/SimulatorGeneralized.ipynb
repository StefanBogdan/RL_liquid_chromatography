{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from chromatography import *\n",
    "from separation_utility import *\n",
    "from torch import optim, tensor\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alists = []\n",
    "alists.append(pd.read_csv(f'../data/GilarSample.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Peterpeptides.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Roca.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Peter32.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Eosin.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Alizarin.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Controlmix2.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_field(exp, taus, N = 200):\n",
    "    phis = np.linspace(0, 1, N)\n",
    "    losses = np.zeros((N, N))\n",
    "    j = 0\n",
    "    for phi1 in phis:\n",
    "        i = 0\n",
    "        for phi2 in phis:\n",
    "            exp.reset()\n",
    "            exp.run_all([phi1, phi2], taus)\n",
    "            losses[i, j] = exp.loss()\n",
    "            i += 1\n",
    "        j += 1\n",
    "    X, Y = np.meshgrid(phis, phis)\n",
    "    \n",
    "    return X, Y, losses\n",
    "\n",
    "def average_over_equal_intervals(arr, interval):\n",
    "    return np.mean(arr.reshape(-1, interval), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5869235063663065, epoch: 20000/20000\n",
      "Loss: 0.5262232719072418, epoch: 20000/20000\n",
      "Loss: 0.37378436537904713, epoch: 20000/20000\n",
      "Loss: 1.088515022493085, epoch: 20000/20000\n",
      "Loss: 0.5594674041101825, epoch: 20000/20000\n",
      "Loss: 0.527469926088541, epoch: 20000/20000\n",
      "Loss: 0.4682274275329287, epoch: 20000/20000\n",
      "Loss: 1.2958575798470886, epoch: 20000/20000\n",
      "Loss: 0.5456559648204672, epoch: 20000/20000\n",
      "Loss: 0.5173011677878424, epoch: 20000/20000\n"
     ]
    }
   ],
   "source": [
    "taus = [.25, .25, 10]\n",
    "L5_b1 = []\n",
    "PL5_b1 = []\n",
    "for i in range(10):\n",
    "    pol = PolicyGeneral(\n",
    "                phi = nn.Sequential(\n",
    "                    PermEqui2_max(2, 5),\n",
    "                    nn.ELU(inplace=True),\n",
    "                    PermEqui2_max(5, 5),\n",
    "                    nn.ELU(inplace=True),\n",
    "                    PermEqui2_max(5, 5),\n",
    "                    nn.ELU(inplace=True),\n",
    "                ),\n",
    "                rho = nn.Sequential(\n",
    "                    nn.Linear(5, 5),\n",
    "                    nn.ELU(inplace=True),\n",
    "                    nn.Linear(5, 5),\n",
    "                    nn.ELU(inplace=True),\n",
    "                    Rho(n_steps=len(taus), hidden=5, in_dim=5, sigma_max=.3, sigma_min=.01),\n",
    "                )\n",
    "            )\n",
    "    l, p = reinforce_gen(\n",
    "        alists = alists[1:], \n",
    "        policy = pol, \n",
    "        delta_taus = taus, \n",
    "        num_episodes = 20_000, \n",
    "        sample_size = 10,\n",
    "        batch_size = 1, \n",
    "        lr = .05, \n",
    "        optim = lambda a, b: torch.optim.SGD(a, b),\n",
    "        lr_decay_factor = 0.75,\n",
    "        lr_milestones = 2500,\n",
    "        print_every = 20_000,\n",
    "        baseline = .55,\n",
    "        max_norm = 1.5,\n",
    "        max_rand_analytes = 30,\n",
    "        min_rand_analytes = 15,\n",
    "        rand_prob = 0.7\n",
    "    )\n",
    "    L5_b1.append(l)\n",
    "    PL5_b1.append(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_over_equal_intervals(np.array(L8).mean(0), 500), label=\"L8\")\n",
    "plt.title(\"Loss (average of 500 random sets)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "exp = ExperimentAnalytes(k0 = alists[i].k0.values, S = alists[i].S.values, h=0.001,run_time=10.0)\n",
    "mu, sig = pol(torch.Tensor(alists[i][['S', 'lnk0']].values))\n",
    "exp.run_all(mu.detach().numpy(), taus)\n",
    "\n",
    "exp.print_analytes(title=f\"Solvent Strength Program(Gen)\\nLoss:{round(exp.loss(), 4)}\", rc=(10,10), angle=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, B, mus, sigmas = reinforce_single_from_gen(\n",
    "        alist=alists[i], \n",
    "        policy=pol, \n",
    "        delta_taus=taus, \n",
    "        num_episodes=3000, \n",
    "        batch_size=10, \n",
    "        lr=.1, \n",
    "        optim=torch.optim.SGD,\n",
    "        lr_decay_factor=.5,\n",
    "        lr_milestones=500,\n",
    "        print_every=500,\n",
    "        baseline=0.65,\n",
    "        max_norm=1.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ExperimentAnalytes(k0 = alists[i].k0.values, S = alists[i].S.values, h=0.001,run_time=10.0)\n",
    "exp.run_all(mus[-1,:], taus)\n",
    "exp.print_analytes(title=f\"Solvent Strength Program(Iso)\\nLoss:{round(exp.loss(), 4)}\", rc=(10,10), angle=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ExperimentAnalytes(k0 = alists[i].k0.values, S = alists[i].S.values, h=0.001,run_time=10.0)\n",
    "exp.run_all(B, taus)\n",
    "exp.print_analytes(title=f\"Best Solvent Strength Program\\nLoss:{round(exp.loss(), 4)}\", rc=(10,10), angle=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = PolicyGeneral(\n",
    "            phi = nn.Sequential(\n",
    "                PermEqui2_max(2, 3),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(3, 3),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(3, 3),\n",
    "                nn.ELU(inplace=True),\n",
    "            ),\n",
    "            rho = RhoTime(n_steps=3, hidden=5, in_dim=3, sigma_max=.3, sigma_min=.02)\n",
    "        )\n",
    "losses = reinforce_delta_tau_gen(\n",
    "    alists = alists, \n",
    "    policy = pol,\n",
    "    num_episodes = 10000, \n",
    "    batch_size = 10, \n",
    "    lr = .1, \n",
    "    optim = lambda a, b: torch.optim.SGD(a, b),\n",
    "    lr_decay_factor= 0.75,\n",
    "    lr_milestones=1000,\n",
    "    print_every = 100,\n",
    "    baseline = .55,\n",
    "    max_norm = 1.2,\n",
    "    max_rand_analytes = 35,\n",
    "    min_rand_analytes = 18,\n",
    "    rand_prob = 0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 200000, 200),average_over_equal_intervals(losses[0], 1000))\n",
    "plt.title(\"Loss [variable delta tau] (average of 1000 random sets)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6\n",
    "exp = ExperimentAnalytes(k0 = alists[i].k0.values, S = alists[i].S.values, h=0.001,run_time=10.0)\n",
    "mu, _ = pol(torch.Tensor(alists[i][['S', 'lnk0']].values))\n",
    "mu = mu.tolist()\n",
    "mu.append(10.)\n",
    "exp.run_all(mu[0:3], mu[3:])\n",
    "\n",
    "exp.print_analytes(title=f\"Solvent Strength Program\\nLoss:{round(exp.loss(), 4)}\", rc=(10,10), angle=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, B, mus, sigmas = reinforce_single_from_delta_tau_gen(\n",
    "        alist=alists[i], \n",
    "        policy=pol,\n",
    "        num_episodes=5000, \n",
    "        batch_size=10, \n",
    "        lr=.1, \n",
    "        optim=torch.optim.SGD,\n",
    "        lr_decay_factor=.5,\n",
    "        lr_milestones=500,\n",
    "        print_every=500,\n",
    "        baseline=0.65,\n",
    "        max_norm=1.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ExperimentAnalytes(k0 = alists[i].k0.values, S = alists[i].S.values, h=0.001,run_time=10.0)\n",
    "mu = mus[-1].tolist()\n",
    "mu.append(10.)\n",
    "exp.run_all(mu[0:3], mu[3:])\n",
    "\n",
    "exp.print_analytes(title=f\"Solvent Strength Program\\nLoss:{round(exp.loss(), 4)}\", rc=(10,10), angle=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGeneralISO(nn.Module):\n",
    "    def __init__(self, \n",
    "            phi: nn.Module,\n",
    "            rho: nn.Module\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        Constructor for PolicyTime torch Module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        phi: nn.Module\n",
    "            The network that encodes the analyte set to a single \n",
    "            vector (embedding)\n",
    "        rho: nn.Module\n",
    "            The network that outputs the programe for separation\n",
    "            returns mean and standard deviation of the action space\n",
    "\n",
    "        Ex:\n",
    "        For a 4 step solvent gradient programe the generalized policy \n",
    "        with 3 elements embedding for the analyte set and intermediate\n",
    "        layers of 5 neurons.\n",
    "        policy = PolicyGeneral(\n",
    "            phi = nn.Sequential(\n",
    "                PermEqui1_max(2, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui1_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui1_max(5, 3),\n",
    "                nn.ELU(inplace=True),\n",
    "            ),\n",
    "            rho = Rho(4, 5, 3, .3, .05)\n",
    "        )\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.phi = phi\n",
    "        self.rho = rho\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        phi_output = self.phi(x)\n",
    "        sum_output = phi_output.sum(0, keepdim=True)\n",
    "        sum_output = torch.cat([sum_output, y], 1)\n",
    "        mu, sigma = self.rho(sum_output)\n",
    "        return mu, sigma\n",
    "\n",
    "#############################################################################\n",
    "def reinforce_gen_iso(\n",
    "        alists: Iterable[pd.DataFrame],\n",
    "        policy: PolicyGeneral, \n",
    "        delta_taus: Iterable[float], \n",
    "        num_episodes: int = 1000, \n",
    "        sample_size: int = 10,\n",
    "        batch_size : int = 10,\n",
    "        lr: float = 1., \n",
    "        optim = torch.optim.SGD,\n",
    "        lr_decay_factor: float = 1.,\n",
    "        lr_milestones: Union[int, Iterable[int]] = 1000,\n",
    "        rand_prob: float = .2,\n",
    "        max_rand_analytes: int = 30,\n",
    "        min_rand_analytes: int = 10,\n",
    "        print_every: int = 100,\n",
    "        baseline: float = 0.,\n",
    "        max_norm: float = None,\n",
    "        beta: float = .0,\n",
    "        weights: list = [1., 1.]\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Run Reinforcement Learning for a single set learning.\n",
    "\n",
    "    alists: Iterable[pd.DataFrame]\n",
    "        A list with pd.Dataframes for each dataset used to train on. \n",
    "    policy: PolicyGeneral\n",
    "        The policy that learns the optimal values for the solvent\n",
    "        strength program.\n",
    "    delta_taus: Iterable[float]\n",
    "        Iterable list with the points of solvent strength change.\n",
    "        MUST be the same length as policy.n_steps\n",
    "    num_episodes = 1000\n",
    "        Number of learning steps.\n",
    "    sample_size = 10\n",
    "        Number of samples taken from the action distribution to perform \n",
    "        Expected loss for the distribution of actions.\n",
    "    batch_size:\n",
    "        Number of experiments to run in order to aproximate the true gradient.\n",
    "    lr = 1.\n",
    "        Learning rate.\n",
    "    optim = torch.optim.SGD\n",
    "        Optimizer that performs weight update using gradients.\n",
    "        By defauld is Stochastic Gradient Descent.\n",
    "    lr_decay_factor: float\n",
    "        Learning rate decay factor used for the LRScheduler.\n",
    "        lr is updated according to lr = lr ** lr_decay_factor.\n",
    "    lr_milestones: Union[int, Iterable[int]]\n",
    "        Milestone episode/s to update the learning rate.\n",
    "        If it is int StepLR is used where lr is changed every lr_milestones.\n",
    "        If it is a list of ints then at that specific episode the lr\n",
    "        will be changed.\n",
    "    rand_prob: float = .2\n",
    "        The probability to draw a random subset from all the analytes.\n",
    "        1 - rand_prob is the probability to use a \"real\" set (provided in\n",
    "        alists).\n",
    "    max_rand_analytes: int = 30\n",
    "        The maximum number of analytes in the randomly drawn set.\n",
    "    min_rand_analytes: int = 10\n",
    "        The minimum number of analytes in the randomly drawn set.\n",
    "    print_every = 100,\n",
    "        Number of episodes to print the average loss on.\n",
    "    weights = [1., 1.]\n",
    "        Weigths of the errors to consider, first one is for the Placement Error,\n",
    "        second one is for Overlap Error, By default both have the same wights.\n",
    "    baseline = 0.\n",
    "        Baseline value for the REINFORCE algorithm.\n",
    "    max_norm = None\n",
    "        Maximal value for the Neural Network Norm2.\n",
    "    beta = .0\n",
    "        Entropy Regularization term, is used for more exploration.\n",
    "        By defauld is disabled.\n",
    "    Returns\n",
    "    -------\n",
    "    (losses, best_program, mus, sigmas)\n",
    "    losses: np.ndarray\n",
    "        Expected loss of the action distribution over the whole learning\n",
    "        process.\n",
    "    \"\"\"\n",
    "\n",
    "    losses = []\n",
    "    perfect_loss = []\n",
    "    exps = []\n",
    "\n",
    "    # Make ExperimentAnalytes object for the given analyte sets for time saving purpose\n",
    "    for alist in alists:\n",
    "        exps.append(ExperimentAnalytes(k0 = alist.k0.values, S = alist.S.values, h=0.001, run_time=10.0))\n",
    "\n",
    "    num_exps = len(alists)\n",
    "\n",
    "    all_analytes = pd.concat(alists, sort=True)[['k0', 'S', 'lnk0']]\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim(policy.parameters(), lr)\n",
    "\n",
    "    # LR sheduler\n",
    "    if isinstance(lr_milestones, list) or isinstance(lr_milestones, np.ndarray):\n",
    "        scheduler = MultiStepLR(optimizer, lr_milestones, gamma=lr_decay_factor)\n",
    "    else:\n",
    "        scheduler = StepLR(optimizer, lr_milestones, gamma=lr_decay_factor)\n",
    "\n",
    "    J_batch = 0\n",
    "\n",
    "    for n in range(num_episodes):\n",
    "        # the set to use for the experiment.\n",
    "        if random() < rand_prob:\n",
    "            dataframe = all_analytes.sample(randint(min_rand_analytes, max_rand_analytes))\n",
    "            input_data = torch.tensor(dataframe[['S', 'lnk0']].values, dtype=torch.float32)\n",
    "            exp = ExperimentAnalytes(k0 = dataframe.k0.values, S = dataframe.S.values, h=0.001, run_time=10.0)\n",
    "\n",
    "        else:\n",
    "            # Choose a random set\n",
    "            set_index = randint(0, num_exps - 1) \n",
    "            exp = exps[set_index]\n",
    "            input_data = torch.tensor(alists[set_index][['S', 'lnk0']].values, dtype=torch.float32)\n",
    "        \n",
    "        expected_loss = 10\n",
    "        for phi in np.linspace(0, 1, 100):\n",
    "            exp.reset()\n",
    "            exp.step(phi, 1.)\n",
    "            if exp.loss() < expected_loss:\n",
    "                phi_iso = phi\n",
    "                expected_loss = exp.loss()    \n",
    "        \n",
    "        # compute distribution parameters (Normal)\n",
    "        mu, sigma = policy.forward(input_data, torch.tensor([[phi_iso]]))\n",
    "\n",
    "        # Sample some values from the actions distributions\n",
    "        programs = sample(mu, sigma, sample_size)\n",
    "        \n",
    "        # Fit the sampled data to the constraint [0,1]\n",
    "        constr_programs = programs.clone()\n",
    "        constr_programs[constr_programs > 1] = 1\n",
    "        constr_programs[constr_programs < 0] = 0\n",
    "        \n",
    "        J = 0\n",
    "        expected_loss = 0\n",
    "        for i in range(sample_size):\n",
    "            exp.reset()            \n",
    "            exp.run_all(constr_programs[i].data.numpy(), delta_taus)\n",
    "\n",
    "            error = exp.loss(weights)\n",
    "            expected_loss += error\n",
    "            log_prob_ = log_prob(programs[i], mu, sigma)\n",
    "            J += (error - baseline) * log_prob_ - beta * torch.exp(log_prob_) * log_prob_\n",
    "        \n",
    "        losses.append(expected_loss/sample_size)\n",
    "        perfect_loss.append(exp.perfect_loss(weights))\n",
    "        if (n + 1) % print_every == 0:\n",
    "            print(f\"Loss: {losses[-1]}, epoch: {n+1}/{num_episodes}\")\n",
    "\n",
    "        J_batch += J/sample_size\n",
    "        if (i + 1) % batch_size == 0:\n",
    "            J_batch /= batch_size\n",
    "            optimizer.zero_grad()\n",
    "            # Calculate gradients\n",
    "            J_batch.backward()\n",
    "\n",
    "            if max_norm:\n",
    "                torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm)\n",
    "\n",
    "            # Apply gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # learning rate decay\n",
    "            scheduler.step()\n",
    "\n",
    "            J_batch = 0\n",
    "        \n",
    "    return np.array(losses), np.array(perfect_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = [.25, .25, 10]\n",
    "L5_b1_iso = []\n",
    "PL5_b1_iso = []\n",
    "for i in range(5):\n",
    "    pol = PolicyGeneralISO(\n",
    "                phi = nn.Sequential(\n",
    "                    PermEqui2_max(2, 5),\n",
    "                    nn.ELU(inplace=True),\n",
    "                    PermEqui2_max(5, 5),\n",
    "                    nn.ELU(inplace=True),\n",
    "                    PermEqui2_max(5, 5),\n",
    "                    nn.ELU(inplace=True),\n",
    "                ),\n",
    "                rho = nn.Sequential(\n",
    "                    nn.Linear(6, 6),\n",
    "                    nn.ELU(inplace=True),\n",
    "                    nn.Linear(6, 6),\n",
    "                    nn.ELU(inplace=True),\n",
    "                    Rho(n_steps=len(taus), hidden=6, in_dim=6, sigma_max=.3, sigma_min=.01),\n",
    "                )\n",
    "            )\n",
    "    l, p = reinforce_gen_iso(\n",
    "        alists = alists, \n",
    "        policy = pol, \n",
    "        delta_taus = taus, \n",
    "        num_episodes = 20_000, \n",
    "        sample_size = 10,\n",
    "        batch_size = 1, \n",
    "        lr = .05, \n",
    "        optim = lambda a, b: torch.optim.SGD(a, b),\n",
    "        lr_decay_factor = 0.75,\n",
    "        lr_milestones = 5000,\n",
    "        print_every = 5000,\n",
    "        baseline = .55,\n",
    "        max_norm = 1.7,\n",
    "        max_rand_analytes = 30,\n",
    "        min_rand_analytes = 10,\n",
    "        rand_prob = 0.7\n",
    "    )\n",
    "    L5_b1_iso.append(l)\n",
    "    PL5_b1_iso.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Simple batch one small')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot((np.array(L5_b1) - np.array(PL5_b1)).reshape((10, 200, 100)).mean(2).T)\n",
    "plt.ylim((0.3, 1))\n",
    "plt.title(\"Simple batch one small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f35bbfbb510>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "#plt.plot((np.array(L5_b1_iso) - np.array(PL5_b1_iso)).mean(0).reshape(-1, 100).mean(1), label = \"ISO\")\n",
    "plt.plot((np.array(L5_b1) - np.array(PL5_b1)).mean(0).reshape(-1, 100).mean(1), label = \"Small NN\")\n",
    "#plt.plot((np.array(L5_b1_big) - np.array(PL5_b1_big)).mean(0).reshape(-1, 100).mean(1), label = \"Big NN\")\n",
    "plt.title(\"Small rho NN vs Big rho NN\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(L5_b1) - np.array(PL5_b1)).mean(0).reshape(-1, 100).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
