{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from chromatography import *\n",
    "from separation_utility import *\n",
    "from torch import optim, tensor\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alists = []\n",
    "alists.append(pd.read_csv('../data/GilarSample.csv'))\n",
    "alists.append(pd.read_csv('../data/Alizarin.csv'))\n",
    "alists.append(pd.read_csv('../data/Peterpeptides.csv'))\n",
    "alists.append(pd.read_csv('../data/Roca.csv'))\n",
    "alists.append(pd.read_csv('../data/Peter32.csv'))\n",
    "alists.append(pd.read_csv('../data/Eosin.csv'))\n",
    "alists.append(pd.read_csv('../data/Controlmix2.csv'))\n",
    "alists.append(pd.read_csv('../data/Gooding.csv'))\n",
    "# GilarSample - 8 analytes\n",
    "# Peterpeptides - 32 analytes\n",
    "# Roca - 14 analytes\n",
    "# Peter32 - 32 analytes\n",
    "# Eosin - 20 analytes\n",
    "# Alizarin - 16 analytes\n",
    "# Controlmix2 - 17 analytes\n",
    "# Gooding - 872 analytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_field(exp, taus, N = 200):\n",
    "    phis = np.linspace(0, 1, N)\n",
    "    losses = np.zeros((N, N))\n",
    "    j = 0\n",
    "    for phi1 in phis:\n",
    "        i = 0\n",
    "        for phi2 in phis:\n",
    "            exp.reset()\n",
    "            exp.run_all([phi1, phi2], taus)\n",
    "            losses[i, j] = exp.loss()\n",
    "            i += 1\n",
    "        j += 1\n",
    "    X, Y = np.meshgrid(phis, phis)\n",
    "    \n",
    "    return X, Y, losses\n",
    "\n",
    "def average_over_equal_intervals(arr, interval):\n",
    "    return np.mean(arr.reshape(-1, interval), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Performance vs n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "all_analytes = pd.concat(alists, sort=True).reset_index()[['k0', 'S', 'lnk0']]\n",
    "\n",
    "kwargs = {\n",
    "    'num_episodes' : 25_000, \n",
    "    'sample_size' : 10,\n",
    "    'batch_size' : 1, \n",
    "    'lr' : .05, \n",
    "    'optim' : torch.optim.SGD,\n",
    "    'lr_decay_factor' : 0.75,\n",
    "    'lr_milestones' : 5000,\n",
    "    'print_every' : 25_001,\n",
    "    'baseline' : .55,\n",
    "    'max_norm' : 1.5,\n",
    "    'max_rand_analytes' : 40,\n",
    "    'min_rand_analytes' : 8,\n",
    "    'rand_prob' : 1.,\n",
    "    'h' : 0.001,\n",
    "    'run_time' : 1.\n",
    "}\n",
    "N = 7\n",
    "M = 15\n",
    "\n",
    "losses_50_50 = np.zeros((N, M, kwargs['num_episodes']))\n",
    "test_losses_50_50 = np.zeros((N, M, kwargs['num_episodes']))\n",
    "losses_100 = np.zeros((N, M, kwargs['num_episodes']))\n",
    "\n",
    "for n in range(0, N):\n",
    "    print(n)\n",
    "    delta_taus = np.ones(n + 1) * 1/(n + 1)\n",
    "    \n",
    "    for i in range(M):\n",
    "        alist_train = all_analytes.sample(frac=0.5)\n",
    "        alist_test = all_analytes.loc[lambda a: ~a.index.isin(alist_train.index.values)]\n",
    "        print(f\"  {i}\")\n",
    "        #Policies\n",
    "        pol_50_50 = PolicyGeneral(\n",
    "            phi = nn.Sequential(\n",
    "                PermEqui2_max(2, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "            ),\n",
    "            rho = nn.Sequential(\n",
    "                nn.Linear(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.Linear(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                Rho(n_steps=len(delta_taus), hidden=5, in_dim=5, sigma_max=.3, sigma_min=.01),\n",
    "            )\n",
    "        )\n",
    "        pol_100 = PolicyGeneral(\n",
    "            phi = nn.Sequential(\n",
    "                PermEqui2_max(2, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "            ),\n",
    "            rho = nn.Sequential(\n",
    "                nn.Linear(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.Linear(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                Rho(n_steps=len(delta_taus), hidden=5, in_dim=5, sigma_max=.3, sigma_min=.01),\n",
    "            )\n",
    "        )\n",
    "        # Run Exp\n",
    "        loss, loss_test = reinforce_gen(\n",
    "            alists = [alist_train], \n",
    "            test_alist = alist_test,\n",
    "            policy = pol_50_50, \n",
    "            delta_taus = delta_taus, \n",
    "            **kwargs\n",
    "        )\n",
    "        loss_100, _ = reinforce_gen(\n",
    "            alists = [all_analytes], \n",
    "            test_alist = None,\n",
    "            policy = pol_100, \n",
    "            delta_taus = delta_taus, \n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        losses_50_50[n,i] = loss\n",
    "        test_losses_50_50[n,i] = loss_test\n",
    "        losses_100[n,i] = loss_100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez_compressed(\"../results/general_perf_vs_n_steps\", losses_50_50=losses_50_50, test_losses_50_50=test_losses_50_50, losses_100=losses_100)\n",
    "np.savez_compressed(\"../results/general_perf_vs_n_steps_losses_50\", losses_50_50=losses_50_50)\n",
    "np.savez_compressed(\"../results/general_perf_vs_n_steps_test_losses_50_50\", test_losses_50_50=test_losses_50_50)\n",
    "np.savez_compressed(\"../results/general_perf_vs_n_steps_losses_100\", losses_100=losses_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance vs number of analytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/Thesis/code/chromatography.py:251: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return delta_tau_phi * (1 + self.k(phi)) / self.k(phi)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "all_analytes = pd.concat(alists, sort=True).reset_index()[['k0', 'S', 'lnk0']]\n",
    "\n",
    "kwargs = {\n",
    "    'num_episodes' : 25_000, \n",
    "    'sample_size' : 10,\n",
    "    'batch_size' : 1, \n",
    "    'lr' : .05, \n",
    "    'optim' : torch.optim.SGD,\n",
    "    'lr_decay_factor' : 0.75,\n",
    "    'lr_milestones' : 5000,\n",
    "    'print_every' : 25_001,\n",
    "    'baseline' : .55,\n",
    "    'max_norm' : 1.5,\n",
    "    'rand_prob' : 1.,\n",
    "    'h' : 0.001,\n",
    "    'run_time' : 1.\n",
    "}\n",
    "N = 5\n",
    "M = 30\n",
    "\n",
    "losses_50_50 = np.zeros((N, M, kwargs['num_episodes']))\n",
    "test_losses_50_50 = np.zeros((N, M, kwargs['num_episodes']))\n",
    "losses_100 = np.zeros((N, M, kwargs['num_episodes']))\n",
    "\n",
    "\n",
    "delta_taus = np.ones(10) * 1/(10)\n",
    "for n in range(N):\n",
    "    for i in range(M):\n",
    "        alist_train = all_analytes.sample(frac=0.5)\n",
    "        alist_test = all_analytes.loc[lambda a: ~a.index.isin(alist_train.index.values)]\n",
    "        print(f\"  {i}\")\n",
    "        #Policies\n",
    "        pol_50_50 = PolicyGeneral(\n",
    "            phi = nn.Sequential(\n",
    "                PermEqui2_max(2, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "            ),\n",
    "            rho = nn.Sequential(\n",
    "                nn.Linear(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.Linear(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                Rho(n_steps=len(delta_taus), hidden=5, in_dim=5, sigma_max=.3, sigma_min=.01),\n",
    "            )\n",
    "        )\n",
    "        pol_100 = PolicyGeneral(\n",
    "            phi = nn.Sequential(\n",
    "                PermEqui2_max(2, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "            ),\n",
    "            rho = nn.Sequential(\n",
    "                nn.Linear(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.Linear(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                Rho(n_steps=len(delta_taus), hidden=5, in_dim=5, sigma_max=.3, sigma_min=.01),\n",
    "            )\n",
    "        )\n",
    "        # Run Exp\n",
    "        loss, loss_test = reinforce_gen(\n",
    "            alists = [alist_train], \n",
    "            test_alist = alist_test,\n",
    "            policy = pol_50_50, \n",
    "            delta_taus = delta_taus,\n",
    "            min_rand_analytes = 8 * (n + 1),\n",
    "            max_rand_analytes = 8 * (n + 1),\n",
    "            **kwargs\n",
    "        )\n",
    "        loss_100, _ = reinforce_gen(\n",
    "            alists = [all_analytes], \n",
    "            test_alist = None,\n",
    "            policy = pol_100, \n",
    "            delta_taus = delta_taus,\n",
    "            min_rand_analytes = 8 * (n + 1),\n",
    "            max_rand_analytes = 8 * (n + 1),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        losses_50_50[n,i] = loss\n",
    "        test_losses_50_50[n,i] = loss_test\n",
    "        losses_100[n,i] = loss_100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"../results/general_perf_vs_nr_analytes_losses_50\", losses_50_50=losses_50_50)\n",
    "np.savez_compressed(\"../results/general_perf_vs_nr_analytes_test_losses_50_50\", test_losses_50_50=test_losses_50_50)\n",
    "np.savez_compressed(\"../results/general_perf_vs_nr_analytes_losses_100\", losses_100=losses_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_over_equal_intervals(arr, interval):\n",
    "    return np.median(arr.reshape(-1, interval), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_50_50 = np.load('../results/general_perf_vs_n_steps_losses_50.npz')['losses_50_50']\n",
    "test_losses_50_50 = np.load('../results/general_perf_vs_n_steps_test_losses_50_50.npz')['test_losses_50_50']\n",
    "losses_100 = np.load('../results/general_perf_vs_n_steps_losses_100.npz')['losses_100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_100  = []\n",
    "for i in range(10):\n",
    "    data_100.append([])\n",
    "    for j in range(20):\n",
    "        data_100[i].append(average_over_equal_intervals(losses_100[i,j], 500))\n",
    "data_100 = np.array(data_100)\n",
    "\n",
    "data_50_50  = []\n",
    "for i in range(10):\n",
    "    data_50_50.append([])\n",
    "    for j in range(20):\n",
    "        data_50_50[i].append(average_over_equal_intervals(losses_50_50[i,j], 500))\n",
    "data_50_50 = np.array(data_50_50)\n",
    "\n",
    "data_50_50_t  = []\n",
    "for i in range(10):\n",
    "    data_50_50_t.append([])\n",
    "    for j in range(20):\n",
    "        data_50_50_t[i].append(average_over_equal_intervals(test_losses_50_50[i,j], 500))\n",
    "data_50_50_t = np.array(data_50_50_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "for i in range(10):\n",
    "\n",
    "    plt.plot(np.linspace(0, 25000, 50),np.median(data_50_50_t, 1)[i], label=\"Test\"+str(i+1), linewidth=2.)\n",
    "    #plt.plot(np.linspace(0, 25000, 50),data_50_50[i], label=\"Train\"+str(i+1))\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
