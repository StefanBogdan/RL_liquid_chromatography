{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from chromatography import *\n",
    "from separation_utility import *\n",
    "from torch import optim, tensor\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alists = []\n",
    "alists.append(pd.read_csv(f'../data/GilarSample.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Peterpeptides.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Roca.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Peter32.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Eosin.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Alizarin.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Controlmix2.csv'))\n",
    "alists.append(pd.read_csv('../data/Gooding.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_field(exp, taus, N = 200):\n",
    "    phis = np.linspace(0, 1, N)\n",
    "    losses = np.zeros((N, N))\n",
    "    j = 0\n",
    "    for phi1 in phis:\n",
    "        i = 0\n",
    "        for phi2 in phis:\n",
    "            exp.reset()\n",
    "            exp.run_all([phi1, phi2], taus)\n",
    "            losses[i, j] = exp.loss()\n",
    "            i += 1\n",
    "        j += 1\n",
    "    X, Y = np.meshgrid(phis, phis)\n",
    "    \n",
    "    return X, Y, losses\n",
    "\n",
    "def average_over_equal_intervals(arr, interval):\n",
    "    return np.mean(arr.reshape(-1, interval), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Performance vs n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "all_analytes = pd.concat(alists, sort=True).reset_index()[['k0', 'S', 'lnk0']]\n",
    "alist_train = all_analytes.sample(frac=0.5)\n",
    "alist_test = all_analytes.loc[lambda a: ~a.index.isin(alist_train.index.values)]\n",
    "\n",
    "kwargs = {\n",
    "    'num_episodes' : 25_000, \n",
    "    'sample_size' : 10,\n",
    "    'batch_size' : 1, \n",
    "    'lr' : .05, \n",
    "    'optim' : torch.optim.SGD,\n",
    "    'lr_decay_factor' : 0.75,\n",
    "    'lr_milestones' : 5000,\n",
    "    'print_every' : 25_001,\n",
    "    'baseline' : .55,\n",
    "    'max_norm' : 1.5,\n",
    "    'max_rand_analytes' : 40,\n",
    "    'min_rand_analytes' : 8,\n",
    "    'rand_prob' : 1.\n",
    "}\n",
    "N = 7\n",
    "M = 15\n",
    "\n",
    "losses_50_50 = np.zeros((N, M, kwargs['num_episodes']))\n",
    "test_losses_50_50 = np.zeros((N, M, kwargs['num_episodes']))\n",
    "losses_100 = np.zeros((N, M, kwargs['num_episodes']))\n",
    "\n",
    "for n in range(0, N):\n",
    "    print(n)\n",
    "    delta_taus = np.linspace(0, 1, n + 3)[1:]\n",
    "    \n",
    "    for i in range(M):\n",
    "        #Policies\n",
    "        pol_50_50 = pol = PolicyGeneral(\n",
    "            phi = nn.Sequential(\n",
    "                PermEqui2_max(2, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "            ),\n",
    "            rho = nn.Sequential(\n",
    "                nn.Linear(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.Linear(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                Rho(n_steps=len(delta_taus), hidden=5, in_dim=5, sigma_max=.3, sigma_min=.01),\n",
    "            )\n",
    "        )\n",
    "        pol_100 = pol = PolicyGeneral(\n",
    "            phi = nn.Sequential(\n",
    "                PermEqui2_max(2, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "            ),\n",
    "            rho = nn.Sequential(\n",
    "                nn.Linear(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.Linear(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                Rho(n_steps=len(delta_taus), hidden=5, in_dim=5, sigma_max=.3, sigma_min=.01),\n",
    "            )\n",
    "        )\n",
    "        # Run Exp\n",
    "        loss, loss_test = reinforce_gen(\n",
    "            alists = [alist_train], \n",
    "            test_alist = alist_test,\n",
    "            policy = pol_50_50, \n",
    "            delta_taus = delta_taus, \n",
    "            **kwargs\n",
    "        )\n",
    "        loss_100, _ = reinforce_gen(\n",
    "            alists = [all_analytes], \n",
    "            test_alist = 1,\n",
    "            policy = pol_100, \n",
    "            delta_taus = delta_taus, \n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        losses_50_50[n,i] = loss\n",
    "        test_losses_50_50[n,i] = loss_test\n",
    "        losses_100[n,i] = loss_100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.39570901, 1.18016017, 1.49267564, 1.53654069, 1.20495505,\n",
       "         1.33999483, 1.48173163, 1.26250608, 1.27515217, 1.28079667,\n",
       "         1.60846818, 1.6601751 , 1.24675248, 1.06937277, 1.22582498,\n",
       "         1.29471249, 1.29031633, 1.0590804 , 1.28333663, 1.32440992,\n",
       "         0.93636637, 1.18462189, 1.09614823, 0.90969964, 0.90298473],\n",
       "        [1.67363713, 1.01803922, 1.51424794, 1.00886063, 1.65379804,\n",
       "         1.17609713, 0.98729732, 1.09655946, 1.65342924, 1.53433914,\n",
       "         1.2433982 , 1.34571868, 1.54250894, 0.99342767, 0.99548167,\n",
       "         1.07631152, 1.03404042, 1.58444198, 1.54167735, 0.55641176,\n",
       "         1.02202308, 1.73103721, 0.65017703, 1.51564514, 0.68051345]],\n",
       "\n",
       "       [[1.26373037, 1.62314679, 1.58959268, 1.66629525, 1.14628947,\n",
       "         1.21382606, 1.34890487, 1.26183351, 1.33679696, 1.47990056,\n",
       "         1.571604  , 1.32086601, 1.31954444, 1.46580132, 1.57856713,\n",
       "         1.57042864, 1.36820375, 1.65573922, 1.18422438, 1.27619634,\n",
       "         0.90698043, 0.64212608, 0.85907478, 0.60045189, 0.6455198 ],\n",
       "        [0.91214952, 0.67165299, 1.43309719, 1.1327747 , 1.08710967,\n",
       "         0.88185357, 1.19124479, 1.29401933, 0.89032102, 1.10652248,\n",
       "         0.89856958, 0.60785646, 1.24266064, 1.4926318 , 0.89049393,\n",
       "         1.05898285, 1.07285488, 0.96085   , 0.89355267, 1.01288881,\n",
       "         0.76871896, 0.81917633, 0.66366657, 0.89115432, 0.9660773 ]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "taus = [.25, .25, 10]\n",
    "\n",
    "time_start_pp = time.perf_counter()\n",
    "exp = ExperimentAnalytes(k0 = alists[0].k0.values, S = alists[0].S.values, h=0.001,run_time=10.0)\n",
    "for i in range(10):\n",
    "    _, _, mus, _ = reinforce_single_from_gen(\n",
    "        alist=alists[0], \n",
    "        policy=pol, \n",
    "        delta_taus=taus, \n",
    "        num_episodes=2000, \n",
    "        sample_size=10, \n",
    "        lr=.05, \n",
    "        optim=torch.optim.SGD,\n",
    "        lr_decay_factor=.5,\n",
    "        lr_milestones=1000,\n",
    "        print_every=100,\n",
    "        baseline=0.55,\n",
    "        max_norm=2.\n",
    "    )\n",
    "    exp.reset()\n",
    "    exp.run_all(mus[-1,:], taus)\n",
    "    \n",
    "time_end_pp = time.perf_counter()\n",
    "# time 523.31177600672"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.514 - (time_end_pp - time_start_pp)/1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(losses_gen_pp, bins=50)\n",
    "plt.title(f\"Final Result Distribution for GenModel (PeterPeptides) Not in Training\")\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(losses_from_gen_pp, bins=50)\n",
    "plt.title(f\"Final Result Distribution for GenModel + FineTune(PeterPeptides) Not in Training\")\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alists[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gillar\n",
    "taus = [.25, .25, 10]\n",
    "pol = PolicyGeneral(\n",
    "            phi = nn.Sequential(\n",
    "                PermEqui2_max(2, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "            ),\n",
    "            rho = nn.Sequential(\n",
    "                nn.Linear(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.Linear(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                Rho(n_steps=len(taus), hidden=5, in_dim=5, sigma_max=.3, sigma_min=.01),\n",
    "            )\n",
    "        )\n",
    "a, b, c, d = reinforce_gen(\n",
    "    alists = alists[:-1], \n",
    "    test_alist = alists[-1],\n",
    "    policy = pol, \n",
    "    delta_taus = taus, \n",
    "    num_episodes = 20_000, \n",
    "    sample_size = 10,\n",
    "    batch_size = 1, \n",
    "    lr = .05, \n",
    "    optim = lambda a, b: torch.optim.SGD(a, b),\n",
    "    lr_decay_factor = 0.75,\n",
    "    lr_milestones = 5000,\n",
    "    print_every = 20_000,\n",
    "    baseline = .55,\n",
    "    max_norm = 1.5,\n",
    "    max_rand_analytes = 30,\n",
    "    min_rand_analytes = 15,\n",
    "    rand_prob = 0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, B, mus, sigmas = reinforce_single_from_gen(\n",
    "        alist=alists[0], \n",
    "        policy=pol, \n",
    "        delta_taus=taus, \n",
    "        num_episodes=2000, \n",
    "        sample_size=10, \n",
    "        lr=.05, \n",
    "        optim=torch.optim.SGD,\n",
    "        lr_decay_factor=.5,\n",
    "        lr_milestones=1000,\n",
    "        print_every=100,\n",
    "        baseline=0.55,\n",
    "        max_norm=2.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = PolicyGeneral(\n",
    "            phi = nn.Sequential(\n",
    "                PermEqui2_max(2, 3),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(3, 3),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui2_max(3, 3),\n",
    "                nn.ELU(inplace=True),\n",
    "            ),\n",
    "            rho = RhoTime(n_steps=3, hidden=5, in_dim=3, sigma_max=.3, sigma_min=.02)\n",
    "        )\n",
    "losses = reinforce_delta_tau_gen(\n",
    "    alists = alists, \n",
    "    policy = pol,\n",
    "    num_episodes = 10000, \n",
    "    batch_size = 10, \n",
    "    lr = .1, \n",
    "    optim = lambda a, b: torch.optim.SGD(a, b),\n",
    "    lr_decay_factor= 0.75,\n",
    "    lr_milestones=1000,\n",
    "    print_every = 100,\n",
    "    baseline = .55,\n",
    "    max_norm = 1.2,\n",
    "    max_rand_analytes = 35,\n",
    "    min_rand_analytes = 18,\n",
    "    rand_prob = 0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 200000, 200),average_over_equal_intervals(losses[0], 1000))\n",
    "plt.title(\"Loss [variable delta tau] (average of 1000 random sets)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6\n",
    "exp = ExperimentAnalytes(k0 = alists[i].k0.values, S = alists[i].S.values, h=0.001,run_time=10.0)\n",
    "mu, _ = pol(torch.Tensor(alists[i][['S', 'lnk0']].values))\n",
    "mu = mu.tolist()\n",
    "mu.append(10.)\n",
    "exp.run_all(mu[0:3], mu[3:])\n",
    "\n",
    "exp.print_analytes(title=f\"Solvent Strength Program\\nLoss:{round(exp.loss(), 4)}\", rc=(10,10), angle=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, B, mus, sigmas = reinforce_single_from_delta_tau_gen(\n",
    "        alist=alists[i], \n",
    "        policy=pol,\n",
    "        num_episodes=5000, \n",
    "        batch_size=10, \n",
    "        lr=.1, \n",
    "        optim=torch.optim.SGD,\n",
    "        lr_decay_factor=.5,\n",
    "        lr_milestones=500,\n",
    "        print_every=500,\n",
    "        baseline=0.65,\n",
    "        max_norm=1.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ExperimentAnalytes(k0 = alists[i].k0.values, S = alists[i].S.values, h=0.001,run_time=10.0)\n",
    "mu = mus[-1].tolist()\n",
    "mu.append(10.)\n",
    "exp.run_all(mu[0:3], mu[3:])\n",
    "\n",
    "exp.print_analytes(title=f\"Solvent Strength Program\\nLoss:{round(exp.loss(), 4)}\", rc=(10,10), angle=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGeneralISO(nn.Module):\n",
    "    def __init__(self, \n",
    "            phi: nn.Module,\n",
    "            rho: nn.Module\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        Constructor for PolicyTime torch Module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        phi: nn.Module\n",
    "            The network that encodes the analyte set to a single \n",
    "            vector (embedding)\n",
    "        rho: nn.Module\n",
    "            The network that outputs the programe for separation\n",
    "            returns mean and standard deviation of the action space\n",
    "\n",
    "        Ex:\n",
    "        For a 4 step solvent gradient programe the generalized policy \n",
    "        with 3 elements embedding for the analyte set and intermediate\n",
    "        layers of 5 neurons.\n",
    "        policy = PolicyGeneral(\n",
    "            phi = nn.Sequential(\n",
    "                PermEqui1_max(2, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui1_max(5, 5),\n",
    "                nn.ELU(inplace=True),\n",
    "                PermEqui1_max(5, 3),\n",
    "                nn.ELU(inplace=True),\n",
    "            ),\n",
    "            rho = Rho(4, 5, 3, .3, .05)\n",
    "        )\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.phi = phi\n",
    "        self.rho = rho\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        phi_output = self.phi(x)\n",
    "        sum_output = phi_output.sum(0, keepdim=True)\n",
    "        sum_output = torch.cat([sum_output, y], 1)\n",
    "        mu, sigma = self.rho(sum_output)\n",
    "        return mu, sigma\n",
    "\n",
    "#############################################################################\n",
    "def reinforce_gen_iso(\n",
    "        alists: Iterable[pd.DataFrame],\n",
    "        policy: PolicyGeneral, \n",
    "        delta_taus: Iterable[float], \n",
    "        num_episodes: int = 1000, \n",
    "        sample_size: int = 10,\n",
    "        batch_size : int = 10,\n",
    "        lr: float = 1., \n",
    "        optim = torch.optim.SGD,\n",
    "        lr_decay_factor: float = 1.,\n",
    "        lr_milestones: Union[int, Iterable[int]] = 1000,\n",
    "        rand_prob: float = .2,\n",
    "        max_rand_analytes: int = 30,\n",
    "        min_rand_analytes: int = 10,\n",
    "        print_every: int = 100,\n",
    "        baseline: float = 0.,\n",
    "        max_norm: float = None,\n",
    "        beta: float = .0,\n",
    "        weights: list = [1., 1.]\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Run Reinforcement Learning for a single set learning.\n",
    "\n",
    "    alists: Iterable[pd.DataFrame]\n",
    "        A list with pd.Dataframes for each dataset used to train on. \n",
    "    policy: PolicyGeneral\n",
    "        The policy that learns the optimal values for the solvent\n",
    "        strength program.\n",
    "    delta_taus: Iterable[float]\n",
    "        Iterable list with the points of solvent strength change.\n",
    "        MUST be the same length as policy.n_steps\n",
    "    num_episodes = 1000\n",
    "        Number of learning steps.\n",
    "    sample_size = 10\n",
    "        Number of samples taken from the action distribution to perform \n",
    "        Expected loss for the distribution of actions.\n",
    "    batch_size:\n",
    "        Number of experiments to run in order to aproximate the true gradient.\n",
    "    lr = 1.\n",
    "        Learning rate.\n",
    "    optim = torch.optim.SGD\n",
    "        Optimizer that performs weight update using gradients.\n",
    "        By defauld is Stochastic Gradient Descent.\n",
    "    lr_decay_factor: float\n",
    "        Learning rate decay factor used for the LRScheduler.\n",
    "        lr is updated according to lr = lr ** lr_decay_factor.\n",
    "    lr_milestones: Union[int, Iterable[int]]\n",
    "        Milestone episode/s to update the learning rate.\n",
    "        If it is int StepLR is used where lr is changed every lr_milestones.\n",
    "        If it is a list of ints then at that specific episode the lr\n",
    "        will be changed.\n",
    "    rand_prob: float = .2\n",
    "        The probability to draw a random subset from all the analytes.\n",
    "        1 - rand_prob is the probability to use a \"real\" set (provided in\n",
    "        alists).\n",
    "    max_rand_analytes: int = 30\n",
    "        The maximum number of analytes in the randomly drawn set.\n",
    "    min_rand_analytes: int = 10\n",
    "        The minimum number of analytes in the randomly drawn set.\n",
    "    print_every = 100,\n",
    "        Number of episodes to print the average loss on.\n",
    "    weights = [1., 1.]\n",
    "        Weigths of the errors to consider, first one is for the Placement Error,\n",
    "        second one is for Overlap Error, By default both have the same wights.\n",
    "    baseline = 0.\n",
    "        Baseline value for the REINFORCE algorithm.\n",
    "    max_norm = None\n",
    "        Maximal value for the Neural Network Norm2.\n",
    "    beta = .0\n",
    "        Entropy Regularization term, is used for more exploration.\n",
    "        By defauld is disabled.\n",
    "    Returns\n",
    "    -------\n",
    "    (losses, best_program, mus, sigmas)\n",
    "    losses: np.ndarray\n",
    "        Expected loss of the action distribution over the whole learning\n",
    "        process.\n",
    "    \"\"\"\n",
    "\n",
    "    losses = []\n",
    "    perfect_loss = []\n",
    "    exps = []\n",
    "\n",
    "    # Make ExperimentAnalytes object for the given analyte sets for time saving purpose\n",
    "    for alist in alists:\n",
    "        exps.append(ExperimentAnalytes(k0 = alist.k0.values, S = alist.S.values, h=0.001, run_time=10.0))\n",
    "\n",
    "    num_exps = len(alists)\n",
    "\n",
    "    all_analytes = pd.concat(alists, sort=True)[['k0', 'S', 'lnk0']]\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim(policy.parameters(), lr)\n",
    "\n",
    "    # LR sheduler\n",
    "    if isinstance(lr_milestones, list) or isinstance(lr_milestones, np.ndarray):\n",
    "        scheduler = MultiStepLR(optimizer, lr_milestones, gamma=lr_decay_factor)\n",
    "    else:\n",
    "        scheduler = StepLR(optimizer, lr_milestones, gamma=lr_decay_factor)\n",
    "\n",
    "    J_batch = 0\n",
    "\n",
    "    for n in range(num_episodes):\n",
    "        # the set to use for the experiment.\n",
    "        if random() < rand_prob:\n",
    "            dataframe = all_analytes.sample(randint(min_rand_analytes, max_rand_analytes))\n",
    "            input_data = torch.tensor(dataframe[['S', 'lnk0']].values, dtype=torch.float32)\n",
    "            exp = ExperimentAnalytes(k0 = dataframe.k0.values, S = dataframe.S.values, h=0.001, run_time=10.0)\n",
    "\n",
    "        else:\n",
    "            # Choose a random set\n",
    "            set_index = randint(0, num_exps - 1) \n",
    "            exp = exps[set_index]\n",
    "            input_data = torch.tensor(alists[set_index][['S', 'lnk0']].values, dtype=torch.float32)\n",
    "        \n",
    "        expected_loss = 10\n",
    "        for phi in np.linspace(0, 1, 100):\n",
    "            exp.reset()\n",
    "            exp.step(phi, 1.)\n",
    "            if exp.loss() < expected_loss:\n",
    "                phi_iso = phi\n",
    "                expected_loss = exp.loss()    \n",
    "        \n",
    "        # compute distribution parameters (Normal)\n",
    "        mu, sigma = policy.forward(input_data, torch.tensor([[phi_iso]]))\n",
    "\n",
    "        # Sample some values from the actions distributions\n",
    "        programs = sample(mu, sigma, sample_size)\n",
    "        \n",
    "        # Fit the sampled data to the constraint [0,1]\n",
    "        constr_programs = programs.clone()\n",
    "        constr_programs[constr_programs > 1] = 1\n",
    "        constr_programs[constr_programs < 0] = 0\n",
    "        \n",
    "        J = 0\n",
    "        expected_loss = 0\n",
    "        for i in range(sample_size):\n",
    "            exp.reset()            \n",
    "            exp.run_all(constr_programs[i].data.numpy(), delta_taus)\n",
    "\n",
    "            error = exp.loss(weights)\n",
    "            expected_loss += error\n",
    "            log_prob_ = log_prob(programs[i], mu, sigma)\n",
    "            J += (error - baseline) * log_prob_ - beta * torch.exp(log_prob_) * log_prob_\n",
    "        \n",
    "        losses.append(expected_loss/sample_size)\n",
    "        perfect_loss.append(exp.perfect_loss(weights))\n",
    "        if (n + 1) % print_every == 0:\n",
    "            print(f\"Loss: {losses[-1]}, epoch: {n+1}/{num_episodes}\")\n",
    "\n",
    "        J_batch += J/sample_size\n",
    "        if (i + 1) % batch_size == 0:\n",
    "            J_batch /= batch_size\n",
    "            optimizer.zero_grad()\n",
    "            # Calculate gradients\n",
    "            J_batch.backward()\n",
    "\n",
    "            if max_norm:\n",
    "                torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm)\n",
    "\n",
    "            # Apply gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # learning rate decay\n",
    "            scheduler.step()\n",
    "\n",
    "            J_batch = 0\n",
    "        \n",
    "    return np.array(losses), np.array(perfect_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = [.25, .25, 10]\n",
    "L5_b1_iso = []\n",
    "PL5_b1_iso = []\n",
    "for i in range(5):\n",
    "    pol = PolicyGeneralISO(\n",
    "                phi = nn.Sequential(\n",
    "                    PermEqui2_max(2, 5),\n",
    "                    nn.ELU(inplace=True),\n",
    "                    PermEqui2_max(5, 5),\n",
    "                    nn.ELU(inplace=True),\n",
    "                    PermEqui2_max(5, 5),\n",
    "                    nn.ELU(inplace=True),\n",
    "                ),\n",
    "                rho = nn.Sequential(\n",
    "                    nn.Linear(6, 6),\n",
    "                    nn.ELU(inplace=True),\n",
    "                    nn.Linear(6, 6),\n",
    "                    nn.ELU(inplace=True),\n",
    "                    Rho(n_steps=len(taus), hidden=6, in_dim=6, sigma_max=.3, sigma_min=.01),\n",
    "                )\n",
    "            )\n",
    "    l, p = reinforce_gen_iso(\n",
    "        alists = alists, \n",
    "        policy = pol, \n",
    "        delta_taus = taus, \n",
    "        num_episodes = 20_000, \n",
    "        sample_size = 10,\n",
    "        batch_size = 1, \n",
    "        lr = .05, \n",
    "        optim = lambda a, b: torch.optim.SGD(a, b),\n",
    "        lr_decay_factor = 0.75,\n",
    "        lr_milestones = 5000,\n",
    "        print_every = 5000,\n",
    "        baseline = .55,\n",
    "        max_norm = 1.7,\n",
    "        max_rand_analytes = 30,\n",
    "        min_rand_analytes = 10,\n",
    "        rand_prob = 0.7\n",
    "    )\n",
    "    L5_b1_iso.append(l)\n",
    "    PL5_b1_iso.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot((np.array(L5_b1) - np.array(PL5_b1)).reshape((10, 200, 100)).mean(2).T)\n",
    "plt.ylim((0.3, 1))\n",
    "plt.title(\"Simple batch one small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "#plt.plot((np.array(L5_b1_iso) - np.array(PL5_b1_iso)).mean(0).reshape(-1, 100).mean(1), label = \"ISO\")\n",
    "plt.plot((np.array(L5_b1) - np.array(PL5_b1)).mean(0).reshape(-1, 100).mean(1), label = \"Small NN\")\n",
    "#plt.plot((np.array(L5_b1_big) - np.array(PL5_b1_big)).mean(0).reshape(-1, 100).mean(1), label = \"Big NN\")\n",
    "plt.title(\"Small rho NN vs Big rho NN\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(L5_b1) - np.array(PL5_b1)).mean(0).reshape(-1, 100).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
