{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from chromatography import ExperimentAnalytes\n",
    "from separation_utility import *\n",
    "from torch import optim, tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alists = []\n",
    "alists.append(pd.read_csv(f'../data/GilarSample.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Alizarin.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Peterpeptides.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Roca.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Peter32.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Eosin.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Controlmix2.csv'))\n",
    "# GilarSample - 8 analytes\n",
    "# Peterpeptides - 32 analytes\n",
    "# Roca - 14 analytes\n",
    "# Peter32 - 32 analytes\n",
    "# Eosin - 20 analytes\n",
    "# Alizarin - 16 analytes\n",
    "# Controlmix2 - 17 analytes\n",
    "# Gooding - 872 analytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_field(exp, taus, N = 200):\n",
    "    phis = np.linspace(0, 1, N)\n",
    "    losses = np.zeros((N, N))\n",
    "    j = 0\n",
    "    for phi1 in phis:\n",
    "        i = 0\n",
    "        for phi2 in phis:\n",
    "            exp.reset()\n",
    "            exp.run_all([phi1, phi2], taus)\n",
    "            losses[i, j] = exp.loss()\n",
    "            i += 1\n",
    "        j += 1\n",
    "    X, Y = np.meshgrid(phis, phis)\n",
    "    \n",
    "    return X, Y, losses\n",
    "def average_over_equal_intervals(arr, interval):\n",
    "    return np.mean(arr.reshape(-1, interval), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Performance vs n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sigma_max = 0.3\n",
    "\n",
    "kwargs = {\n",
    "'num_episodes' : 6000,\n",
    "'sample_size' : 10,\n",
    "'lr' : .05,\n",
    "'optim' : torch.optim.SGD,\n",
    "'lr_decay_factor' : .5,\n",
    "'lr_milestones' : 1000,\n",
    "'print_every' : 6001,\n",
    "'baseline' : 0.55,\n",
    "'max_norm' : 2.\n",
    "}\n",
    "N = 10\n",
    "M = 50\n",
    "# Experiments\n",
    "exp_8 = ExperimentAnalytes(k0 = alists[0].k0.values, S = alists[0].S.values, h=0.001, run_time=1.0)\n",
    "exp_16 = ExperimentAnalytes(k0 = alists[1].k0.values, S = alists[1].S.values, h=0.001, run_time=1.0)\n",
    "exp_32 = ExperimentAnalytes(k0 = alists[2].k0.values, S = alists[2].S.values, h=0.001, run_time=1.0)\n",
    "# Final Results \n",
    "dist_8 = np.zeros((N, M))\n",
    "dist_16 = np.zeros((N, M))\n",
    "dist_32 = np.zeros((N, M))\n",
    "\n",
    "for n in range(0, N):\n",
    "    print(n)\n",
    "    delta_taus = np.ones(n + 1) * 1/(n + 1)\n",
    "    \n",
    "    for i in range(M):\n",
    "        print(f\"  {i}\")\n",
    "        #Policies\n",
    "        pol_8 = PolicySingle(len(delta_taus), sigma_max = sigma_max)\n",
    "        pol_16 = PolicySingle(len(delta_taus), sigma_max = sigma_max)\n",
    "        pol_32 = PolicySingle(len(delta_taus), sigma_max = sigma_max)\n",
    "        # Run Exp 8\n",
    "        reinforce_one_set(\n",
    "                exp_8, \n",
    "                pol_8, \n",
    "                delta_taus=delta_taus, \n",
    "                **kwargs\n",
    "            )\n",
    "        # Run Exp 16\n",
    "        reinforce_one_set(\n",
    "                exp_16, \n",
    "                pol_16, \n",
    "                delta_taus=delta_taus, \n",
    "                **kwargs\n",
    "            )\n",
    "        # Run Exp 32\n",
    "        reinforce_one_set(\n",
    "                exp_32, \n",
    "                pol_32, \n",
    "                delta_taus=delta_taus, \n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        exp_8.reset()\n",
    "        exp_16.reset()\n",
    "        exp_32.reset()\n",
    "        mu_8, _ = pol_8.forward()\n",
    "        mu_16, _ = pol_16.forward()\n",
    "        mu_32, _ = pol_32.forward()\n",
    "        exp_8.run_all(mu_8.detach().numpy(), delta_taus)\n",
    "        exp_16.run_all(mu_16.detach().numpy(), delta_taus)\n",
    "        exp_32.run_all(mu_32.detach().numpy(), delta_taus)\n",
    "        \n",
    "        dist_8[n, i] = exp_8.loss()\n",
    "        dist_16[n, i] = exp_16.loss()\n",
    "        dist_32[n, i] = exp_32.loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"../results/distribution_single_perf_vs_n_steps\", nr_analytes_8=dist_8, nr_analytes_16=dist_16, nr_analytes_32=dist_32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: RL vs grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sigma_max = 0.3\n",
    "\n",
    "kwargs = {\n",
    "'num_episodes' : 6000,\n",
    "'sample_size' : 10,\n",
    "'lr' : .05,\n",
    "'optim' : torch.optim.SGD,\n",
    "'lr_decay_factor' : .5,\n",
    "'lr_milestones' : 1000,\n",
    "'print_every' : 6001,\n",
    "'baseline' : 0.55,\n",
    "'max_norm' : 2.\n",
    "}\n",
    "exp = ExperimentAnalytes(k0 = alists[2].k0.values, S = alists[2].S.values, h=0.001, run_time=1.0)\n",
    "\n",
    "### one n_step\n",
    "loss = 3.\n",
    "delta_taus = [1.]\n",
    "print(\"Grid Search 1\")\n",
    "# Grid Search\n",
    "start_gs_1 = time.perf_counter()\n",
    "for phi in np.linspace(0, 1, 100):\n",
    "    exp.reset()\n",
    "    exp.run_all([phi], delta_taus)\n",
    "    if exp.loss() < loss:\n",
    "        loss = exp.loss()\n",
    "        best_1 = [phi]\n",
    "end_gs_1 = time.perf_counter()\n",
    "# RL\n",
    "print(\"RL 1\")\n",
    "start_rl_1 = time.perf_counter()\n",
    "for n in range(10):\n",
    "    pol = PolicySingle(len(delta_taus), sigma_max = sigma_max)\n",
    "    reinforce_one_set(\n",
    "            exp, \n",
    "            pol, \n",
    "            delta_taus=delta_taus, \n",
    "            **kwargs\n",
    "        )\n",
    "end_rl_1 = time.perf_counter()\n",
    "\n",
    "\n",
    "### two n_step\n",
    "loss = 3.\n",
    "delta_taus = [.5, .5]\n",
    "# Grid Search\n",
    "print(\"Grid Search 2\")\n",
    "start_gs_2 = time.perf_counter()\n",
    "for phi_1 in np.linspace(0, 1, 100):\n",
    "    for phi_2 in np.linspace(0, 1, 100):\n",
    "        exp.reset()\n",
    "        exp.run_all([phi_1, phi_2], delta_taus)\n",
    "        if exp.loss() < loss:\n",
    "            loss = exp.loss()\n",
    "            best_2 = [phi_1, phi_2]\n",
    "end_gs_2 = time.perf_counter()\n",
    "# RL\n",
    "print(\"RL 2\")\n",
    "start_rl_2 = time.perf_counter()\n",
    "for n in range(10):\n",
    "    pol = PolicySingle(len(delta_taus), sigma_max = sigma_max)\n",
    "    reinforce_one_set(\n",
    "            exp, \n",
    "            pol, \n",
    "            delta_taus=delta_taus, \n",
    "            **kwargs\n",
    "        )\n",
    "end_rl_2 = time.perf_counter()\n",
    "### three n_step\n",
    "loss = 3.\n",
    "delta_taus = [.33, .33, .34]\n",
    "# Grid Search\n",
    "print(\"Grid Search 3\")\n",
    "start_gs_3 = time.perf_counter()\n",
    "for phi_1 in np.linspace(0, 1, 100):\n",
    "    for phi_2 in np.linspace(0, 1, 100):\n",
    "        for phi_3 in np.linspace(0, 1, 100):\n",
    "            exp.reset()\n",
    "            exp.run_all([phi_1, phi_2, phi_3], delta_taus)\n",
    "            if exp.loss() < loss:\n",
    "                loss = exp.loss()\n",
    "                best_3 = [phi_1, phi_2, phi_3]\n",
    "end_gs_3 = time.perf_counter()\n",
    "# RL\n",
    "print(\"RL 3\")\n",
    "start_rl_3 = time.perf_counter()\n",
    "for n in range(10):\n",
    "    pol = PolicySingle(len(delta_taus), sigma_max = sigma_max)\n",
    "    reinforce_one_set(\n",
    "            exp, \n",
    "            pol, \n",
    "            delta_taus=delta_taus, \n",
    "            **kwargs\n",
    "        )\n",
    "end_rl_3 = time.perf_counter()\n",
    "### four n_step\n",
    "loss = 3.\n",
    "delta_taus = [.25, .25, .25, .25]\n",
    "# Grid Search\n",
    "print(\"Grid Search 4\")\n",
    "start_gs_4 = time.perf_counter()\n",
    "for phi_1 in np.linspace(0, 1, 100):\n",
    "    for phi_2 in np.linspace(0, 1, 100):\n",
    "        for phi_3 in np.linspace(0, 1, 100):\n",
    "            for phi_4 in np.linspace(0, 1, 100):\n",
    "                exp.reset()\n",
    "                exp.run_all([phi_1, phi_2, phi_3, phi_4], delta_taus)\n",
    "                if exp.loss() < loss:\n",
    "                    loss = exp.loss()\n",
    "                    best_4 = [phi_1, phi_2, phi_3, phi_4]\n",
    "end_gs_4 = time.perf_counter()\n",
    "# RL\n",
    "print(\"RL 4\")\n",
    "start_rl_4 = time.perf_counter()\n",
    "for n in range(10):\n",
    "    pol = PolicySingle(len(delta_taus), sigma_max = sigma_max)\n",
    "    reinforce_one_set(\n",
    "            exp, \n",
    "            pol, \n",
    "            delta_taus=delta_taus, \n",
    "            **kwargs\n",
    "        )\n",
    "end_rl_4 = time.perf_counter()\n",
    "\n",
    "grid_search_time_ms = np.array([end_gs_1 - start_gs_1, end_gs_2 - start_gs_2, end_gs_3 - start_gs_3, end_gs_4 - start_gs_4])\n",
    "rl_time_ms = np.array([end_rl_1 - start_rl_1, end_rl_2 - start_rl_2, end_rl_3 - start_rl_3, end_rl_4 - start_rl_4])/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../results/grid_search_time_ms.txt\", grid_search_time_ms)\n",
    "np.savetxt(\"../results/rl_time_ms.txt\", rl_time_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_1)\n",
    "print(best_2)\n",
    "print(best_3)\n",
    "print(best_4)\n",
    "[0.3535353535353536]\n",
    "[0.31313131313131315, 0.38383838383838387]\n",
    "[0.32323232323232326, 0.38383838383838387, 0.37373737373737376]\n",
    "[0.32323232323232326, 0.37373737373737376, 0.38383838383838387, 0.37373737373737376]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: RL vs delta tau RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sigma_max = 0.3\n",
    "\n",
    "kwargs = {\n",
    "'num_episodes' : 6000,\n",
    "'sample_size' : 10,\n",
    "'lr' : .05,\n",
    "'optim' : torch.optim.SGD,\n",
    "'lr_decay_factor' : .5,\n",
    "'lr_milestones' : 1000,\n",
    "'print_every' : 6001,\n",
    "'baseline' : 0.55,\n",
    "'max_norm' : 2.\n",
    "}\n",
    "N = 10\n",
    "M = 50\n",
    "# Experiments\n",
    "exp_8 = ExperimentAnalytes(k0 = alists[0].k0.values, S = alists[0].S.values, h=0.001, run_time=1.0)\n",
    "exp_16 = ExperimentAnalytes(k0 = alists[1].k0.values, S = alists[1].S.values, h=0.001, run_time=1.0)\n",
    "exp_32 = ExperimentAnalytes(k0 = alists[2].k0.values, S = alists[2].S.values, h=0.001, run_time=1.0)\n",
    "# Final Results \n",
    "dist_tau_8 = np.zeros((N, M))\n",
    "dist_tau_16 = np.zeros((N, M))\n",
    "dist_tau_32 = np.zeros((N, M))\n",
    "\n",
    "for n in range(1, N):    \n",
    "    for i in range(M):\n",
    "        print(f\"  {i}\")\n",
    "        #Policies\n",
    "        pol_tau_8 = PolicySingleTime(n + 1, sigma_max = sigma_max)\n",
    "        pol_tau_16 = PolicySingleTime(n + 1, sigma_max = sigma_max)\n",
    "        pol_tau_32 = PolicySingleTime(n + 1, sigma_max = sigma_max)\n",
    "        # Run Exp 8\n",
    "        reinforce_delta_tau(\n",
    "                exp_8, \n",
    "                pol_tau_8,\n",
    "                **kwargs\n",
    "            )\n",
    "        # Run Exp 16\n",
    "        reinforce_delta_tau(\n",
    "                exp_16, \n",
    "                pol_tau_16,\n",
    "                **kwargs\n",
    "            )\n",
    "        # Run Exp 32\n",
    "        reinforce_delta_tau(\n",
    "                exp_32, \n",
    "                pol_tau_32,\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        exp_8.reset()\n",
    "        exp_16.reset()\n",
    "        exp_32.reset()\n",
    "        mu_8, _ = pol_tau_8.forward()\n",
    "        mu_16, _ = pol_tau_16.forward()\n",
    "        mu_32, _ = pol_tau_32.forward()\n",
    "        grads_8, delta_taus_8 = np.split(torch.cat((mu_8, tensor([1.])), 0).data.numpy(), 2)\n",
    "        grads_16, delta_taus_16 = np.split(torch.cat((mu_16, tensor([1.])), 0).data.numpy(), 2)\n",
    "        grads_32, delta_taus_32 = np.split(torch.cat((mu_32, tensor([1.])), 0).data.numpy(), 2)\n",
    "        exp_8.run_all(grads_8, delta_taus_8)\n",
    "        exp_16.run_all(grads_16, delta_taus_16)\n",
    "        exp_32.run_all(grads_32, delta_taus_32)\n",
    "        \n",
    "        dist_tau_8[n, i] = exp_8.loss()\n",
    "        dist_tau_16[n, i] = exp_16.loss()\n",
    "        dist_tau_32[n, i] = exp_32.loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"../results/distribution_single_delta_tau_perf_vs_n_steps\", nr_analytes_8=dist_tau_8, nr_analytes_16=dist_tau_16, nr_analytes_32=dist_tau_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
