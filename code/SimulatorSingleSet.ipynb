{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from chromatography import ExperimentAnalytes\n",
    "from separation_utility import *\n",
    "from torch import optim, tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alists = []\n",
    "alists.append(pd.read_csv(f'../data/GilarSample.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Alizarin.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Peterpeptides.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Roca.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Peter32.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Eosin.csv'))\n",
    "alists.append(pd.read_csv(f'../data/Controlmix2.csv'))\n",
    "# GilarSample - 8 analytes\n",
    "# Peterpeptides - 32 analytes\n",
    "# Roca - 14 analytes\n",
    "# Peter32 - 32 analytes\n",
    "# Eosin - 20 analytes\n",
    "# Alizarin - 16 analytes\n",
    "# Controlmix2 - 17 analytes\n",
    "# Gooding - 872 analytes\n",
    "#alist['k0'] = np.exp(alist.lnk0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_field(exp, taus, N = 200):\n",
    "    phis = np.linspace(0, 1, N)\n",
    "    losses = np.zeros((N, N))\n",
    "    j = 0\n",
    "    for phi1 in phis:\n",
    "        i = 0\n",
    "        for phi2 in phis:\n",
    "            exp.reset()\n",
    "            exp.run_all([phi1, phi2], taus)\n",
    "            losses[i, j] = exp.loss()\n",
    "            i += 1\n",
    "        j += 1\n",
    "    X, Y = np.meshgrid(phis, phis)\n",
    "    \n",
    "    return X, Y, losses\n",
    "def average_over_equal_intervals(arr, interval):\n",
    "    return np.mean(arr.reshape(-1, interval), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sample = 'Peterpeptides'\n",
    "alist = pd.read_csv(f'../data/{sample}.csv')\n",
    "# GilarSample\n",
    "run_time_lim = 10.0\n",
    "sigma_max = 0.3\n",
    "delta_taus = [.25, .25, 10]\n",
    "num_episodes = 5000\n",
    "sample_size = 10\n",
    "lr = .01\n",
    "optim = lambda a, b: torch.optim.SGD(a, b)#, momentum=0.65)\n",
    "lr_decay_factor = .5\n",
    "lr_milestones = 1000\n",
    "print_every= 100\n",
    "baseline = 0.55\n",
    "max_norm = 2.\n",
    "exp = ExperimentAnalytes(k0 = alist.k0.values, S = alist.S.values, h=0.001, run_time=10.0)\n",
    "losses_dist = []\n",
    "\n",
    "start_process_peterp = time.perf_counter()\n",
    "\n",
    "for i in range(50):\n",
    "    pol = PolicySingle(len(delta_taus), sigma_max = sigma_max)\n",
    "    reinforce_one_set(\n",
    "            exp, \n",
    "            pol, \n",
    "            delta_taus= delta_taus, \n",
    "            num_episodes=num_episodes, \n",
    "            sample_size=sample_size,\n",
    "            optim=optim,\n",
    "            lr=lr, \n",
    "            print_every=print_every,\n",
    "            lr_decay_factor=lr_decay_factor,\n",
    "            lr_milestones=lr_milestones,\n",
    "            baseline=baseline,\n",
    "            max_norm=max_norm,\n",
    "\n",
    "        )\n",
    "    exp.reset()\n",
    "    exp.run_all(pol.mu.detach().numpy(), delta_taus)\n",
    "    \n",
    "    losses_dist.append(exp.loss())\n",
    "end_process_peterp = time.perf_counter()\n",
    "# time 1.8 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(end_process_peterp - start_process_peterp)/3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(losses_dist, bins=50)\n",
    "plt.title(f\"Final Result Distribution for SingleSetModel (PeterPeptides)\")\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sample = 'GilarSample'\n",
    "alist = pd.read_csv(f'../data/{sample}.csv')\n",
    "# GilarSample\n",
    "run_time_lim = 10.0\n",
    "sigma_max = 0.3\n",
    "delta_taus = [.25, .25, 10]\n",
    "num_episodes = 5000\n",
    "sample_size = 10\n",
    "lr = .01\n",
    "optim = lambda a, b: torch.optim.SGD(a, b)#, momentum=0.65)\n",
    "lr_decay_factor = .5\n",
    "lr_milestones = 1000\n",
    "print_every= 100\n",
    "baseline = 0.55\n",
    "max_norm = 2.\n",
    "exp = ExperimentAnalytes(k0 = alist.k0.values, S = alist.S.values, h=0.001, run_time=10.0)\n",
    "losses_dist_gil = []\n",
    "\n",
    "start_process_gil = time.perf_counter()\n",
    "\n",
    "for i in range(50):\n",
    "    pol = PolicySingle(len(delta_taus), sigma_max = sigma_max)\n",
    "    reinforce_one_set(\n",
    "            exp, \n",
    "            pol, \n",
    "            delta_taus= delta_taus, \n",
    "            num_episodes=num_episodes, \n",
    "            sample_size=sample_size,\n",
    "            optim=optim,\n",
    "            lr=lr, \n",
    "            print_every=print_every,\n",
    "            lr_decay_factor=lr_decay_factor,\n",
    "            lr_milestones=lr_milestones,\n",
    "            baseline=baseline,\n",
    "            max_norm=max_norm,\n",
    "\n",
    "        )\n",
    "    exp.reset()\n",
    "    exp.run_all(pol.mu.detach().numpy(), delta_taus)\n",
    "    \n",
    "    losses_dist_gil.append(exp.loss())\n",
    "end_process_gil = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(losses_dist_gil, bins=50)\n",
    "plt.title(f\"Final Result Distribution for SingleSetModel (GillarSample)\")\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Performance vs n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_(ab = 10, cd = \"shit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/Thesis/code/chromatography.py:250: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return delta_tau_phi * (1 + self.k(phi)) / self.k(phi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/Thesis/code/chromatography.py:250: RuntimeWarning: overflow encountered in true_divide\n",
      "  return delta_tau_phi * (1 + self.k(phi)) / self.k(phi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e9fb132e5350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mpol_32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mdelta_taus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelta_taus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             )\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Thesis/code/separation_utility.py\u001b[0m in \u001b[0;36mreinforce_one_set\u001b[0;34m(exp, policy, delta_taus, num_episodes, sample_size, lr, optim, lr_decay_factor, lr_milestones, print_every, baseline, max_norm, beta, weights)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;31m# Calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m         \u001b[0mJ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "sigma_max = 0.3\n",
    "\n",
    "kwargs = {\n",
    "'num_episodes' : 6000,\n",
    "'sample_size' : 10,\n",
    "'lr' : .05,\n",
    "'optim' : torch.optim.SGD,\n",
    "'lr_decay_factor' : .5,\n",
    "'lr_milestones' : 1000,\n",
    "'print_every' : 6001,\n",
    "'baseline' : 0.55,\n",
    "'max_norm' : 2.\n",
    "}\n",
    "N = 7\n",
    "M = 100\n",
    "# Experiments\n",
    "exp_8 = ExperimentAnalytes(k0 = alists[0].k0.values, S = alists[0].S.values, h=0.001, run_time=1.0)\n",
    "exp_16 = ExperimentAnalytes(k0 = alists[1].k0.values, S = alists[1].S.values, h=0.001, run_time=1.0)\n",
    "exp_32 = ExperimentAnalytes(k0 = alists[2].k0.values, S = alists[2].S.values, h=0.001, run_time=1.0)\n",
    "# Final Results \n",
    "dist_8 = np.zeros((N, M))\n",
    "dist_16 = np.zeros((N, M))\n",
    "dist_32 = np.zeros((N, M))\n",
    "\n",
    "for n in range(0, N):\n",
    "    print(n)\n",
    "    delta_taus = np.linspace(0, 1, n + 3)[1:]\n",
    "    \n",
    "    for i in range(M):\n",
    "        #Policies\n",
    "        pol_8 = PolicySingle(len(delta_taus), sigma_max = sigma_max)\n",
    "        pol_16 = PolicySingle(len(delta_taus), sigma_max = sigma_max)\n",
    "        pol_32 = PolicySingle(len(delta_taus), sigma_max = sigma_max)\n",
    "        # Run Exp 8\n",
    "        reinforce_one_set(\n",
    "                exp_8, \n",
    "                pol_8, \n",
    "                delta_taus=delta_taus, \n",
    "                **kwargs\n",
    "            )\n",
    "        # Run Exp 16\n",
    "        reinforce_one_set(\n",
    "                exp_16, \n",
    "                pol_16, \n",
    "                delta_taus=delta_taus, \n",
    "                **kwargs\n",
    "            )\n",
    "        # Run Exp 32\n",
    "        reinforce_one_set(\n",
    "                exp_32, \n",
    "                pol_32, \n",
    "                delta_taus=delta_taus, \n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        exp_8.reset()\n",
    "        exp_16.reset()\n",
    "        exp_32.reset()\n",
    "        mu_8, _ = pol_8.forward()\n",
    "        mu_16, _ = pol_16.forward()\n",
    "        mu_32, _ = pol_32.forward()\n",
    "        exp_8.run_all(mu_8.detach().numpy(), delta_taus)\n",
    "        exp_16.run_all(mu_16.detach().numpy(), delta_taus)\n",
    "        exp_32.run_all(mu_32.detach().numpy(), delta_taus)\n",
    "        \n",
    "        dist_8[n, i] = exp_8.loss()\n",
    "        dist_16[n, i] = exp_16.loss()\n",
    "        dist_32[n, i] = exp_32.loss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.reset()\n",
    "exp.run_all(pol.mu.detach().numpy(), delta_taus)\n",
    "exp.print_analytes(rc=(7, 5), angle=45,  title=f'Solvent Strength function\\nLoss: {round(exp.loss(),4)}')\n",
    "plt.show()\n",
    "plt.legend()\n",
    "#plt.savefig(f'results/{sample}_result_{run}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.reset()\n",
    "exp.run_all(best_program, delta_taus)\n",
    "exp.print_analytes(rc=(7, 5), angle=45,  title=f'Solvent Strength function\\nLoss: {round(exp.loss(),4)}')\n",
    "plt.legend()\n",
    "#plt.savefig(f'results/{sample}_best_result_{run}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mus[:, 0], label='Mu: phi1')\n",
    "plt.plot(mus[:, 1], label='Mu: phi2')\n",
    "#plt.plot(mus[:, 2], label='Mu: phi3')\n",
    "#plt.plot(mus[:, 3], label='Mu: phi4')\n",
    "plt.ylim((0,1))\n",
    "plt.legend()\n",
    "#plt.savefig(f'results/{sample}_mus_{run}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigmas[:, 0], label='Sigma: phi1')\n",
    "plt.plot(sigmas[:, 1], label='Sigma: phi2')\n",
    "#plt.plot(sigmas[:, 2], label='Sigma: phi3')\n",
    "#plt.plot(sigmas[:, 3], label='Sigma: phi4')\n",
    "plt.ylim((0,sigma_max))\n",
    "plt.legend()\n",
    "#plt.savefig(f'results/{sample}_sigmas_{run}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss during learning process\")\n",
    "#plt.savefig(f'results/{sample}_loss_{run}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, Loss_field = loss_field(exp, [.25, 30000], N = 500)\n",
    "plt.contourf(X, Y, Loss_field, levels=100)\n",
    "plt.xlabel(\"Phi1\")\n",
    "plt.ylabel(\"Phi2\")\n",
    "index = np.unravel_index(np.argmin(Loss_field), Loss_field.shape)\n",
    "plt.scatter(X[index], Y[index], c=\"r\", s=5)\n",
    "plt.title(f\"Loss Field, change of phi at 0.5\\nGlobal min ({round(np.min(Loss_field), 3)}) at {round(X[index], 4), round(Y[index], 4)}, red dot\")\n",
    "#plt.savefig(f'results/{sample}_loss_filed_at_5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = -1\n",
    "mlab.surf(X.T[:N, :N], Y.T[:N, :N], Loss_field.T[:N, :N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "phis = np.linspace(0, 1, 1000)\n",
    "losses = []\n",
    "for phi1 in phis:\n",
    "    exp.reset()\n",
    "    exp.run_all([phi1], [10])\n",
    "    losses.append(exp.loss())\n",
    "plt.plot(losses[:])\n",
    "plt.ylim((0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(X, Y, Loss_field, levels=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_time = ExperimentAnalytes(k0 = alist.k0.values, S = alist.S.values, h=0.001, grad='iso', run_time=10.0)\n",
    "pol_time = PolicyTime(2, sigma_max = .2)\n",
    "losses_time, best_program_time, mus_time, sigmas_time, n_par_time = reinforce_delta_tau(\n",
    "        exp_time, \n",
    "        pol_time,\n",
    "        num_episodes=5000, \n",
    "        batch_size=10,\n",
    "        optim=lambda a, b: torch.optim.SGD(a, b, momentum=0.65),\n",
    "        lr=.1, \n",
    "        print_every=100,\n",
    "        lr_decay=lambda a, b, c: step_decay(a, b, c, steps=5, decay_factor=0.5),\n",
    "        baseline=0.65,\n",
    "        max_norm=2.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_time.reset()\n",
    "exp_time.run_all(pol_time.mu.detach().numpy()[:2], [0.25, 3000000])\n",
    "exp_time.print_analytes(rc=(7, 5))\n",
    "exp_time.loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_time.reset()\n",
    "exp_time.run_all(best_program_time[0], best_program_time[1])\n",
    "exp_time.print_analytes(rc=(7, 5))\n",
    "exp_time.loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mus_time[:, 0], label='Mu: 0')\n",
    "plt.plot(mus_time[:, 1], label='Mu: 1')\n",
    "plt.plot(mus_time[:, 2], label='Mu: 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigmas_time[:, 0], label='Sigma: 0')\n",
    "plt.plot(sigmas_time[:, 1], label='Sigma: 1')\n",
    "plt.plot(sigmas_time[:, 2], label='Sigma: 2')\n",
    "plt.plot(sigmas_time[:, 3], label='Sigma: 3')\n",
    "plt.plot(sigmas_time[:, 4], label='Sigma: 4')\n",
    "plt.plot(sigmas_time[:, 5], label='Sigma: 5')\n",
    "plt.plot(sigmas_time[:, 6], label='Sigma: 6')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
